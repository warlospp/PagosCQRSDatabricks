{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb525c62-ebef-43e4-be6a-c83288f3ce31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, IntegerType, StringType, DoubleType\n",
    "\n",
    "# 1. Crear sesión de Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaToMongoBatchJob\") \\\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.0,\" \n",
    "            \"org.mongodb.spark:mongo-spark-connector_2.12:10.1.1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 2. Definir esquema del mensaje Kafka\n",
    "schema = StructType() \\\n",
    "    .add(\"Id\", IntegerType()) \\\n",
    "    .add(\"ClienteId\", StringType()) \\\n",
    "    .add(\"Monto\", DoubleType()) \\\n",
    "    .add(\"FechaPago\", StringType()) \\\n",
    "    .add(\"MetodoPago\", StringType()) \\\n",
    "    .add(\"Estado\", StringType())\n",
    "\n",
    "# 3. Leer mensajes desde Kafka (modo batch con trigger 'availableNow')\n",
    "# === Configuración de Kafka (Confluent Cloud) ===\n",
    "kafka_bootstrap_servers = \"pkc-619z3.us-east1.gcp.confluent.cloud:9092\"\n",
    "kafka_topic = \"Pagos\"\n",
    "kafka_sasl_username = \"DJYWPXMBGK457WXF\"\n",
    "kafka_sasl_password = \"STQpFpmDDCYgNjHpeS7aHlYAtnqOIlRimkPk5uwNgo6+JRU5nOafjgDXwURg2QiN\"\n",
    "\n",
    "kafka_options = {\n",
    "    \"kafka.bootstrap.servers\": kafka_bootstrap_servers,\n",
    "    \"subscribe\": kafka_topic,\n",
    "    \"kafka.security.protocol\": \"SASL_SSL\",\n",
    "    \"kafka.sasl.mechanism\": \"PLAIN\",\n",
    "    \"kafka.sasl.jaas.config\": f'org.apache.kafka.common.security.plain.PlainLoginModule required username=\"{kafka_sasl_username}\" password=\"{kafka_sasl_password}\";',\n",
    "    \"startingOffsets\": \"earliest\"\n",
    "}\n",
    "\n",
    "df_kafka_streaming = spark.readStream.format(\"kafka\").options(**kafka_options).load()\n",
    "\n",
    "df_kafka_raw = df_kafka_streaming\n",
    "# Convertir 'value' a string y parsear JSON con el esquema\n",
    "df_parsed = df_kafka_raw.select(\n",
    "    from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\")\n",
    ")\n",
    "# Seleccionar campos del struct 'data' para mostrar columnas separadas\n",
    "df_final = df_parsed.select(\"data.*\")\n",
    "\n",
    "# Renombrar la columna 'Id' a '_id'\n",
    "df_mongo = df_final.withColumnRenamed(\"Id\", \"_id\")\n",
    "\n",
    "# 4. Escribir en MongoDB Atlas\n",
    "mongo_uri = \"mongodb+srv://warlospp:Admin.123@cluster0.w92hpyq.mongodb.net/bddprodserv?retryWrites=true&w=majority\"\n",
    "mongo_db = \"bddprodserv\"\n",
    "mongo_collection = \"Pagos\"\n",
    "\n",
    "df_mongo.writeStream   \\\n",
    "    .format(\"mongodb\") \\\n",
    "    .option(\"spark.mongodb.connection.uri\", mongo_uri) \\\n",
    "    .option(\"spark.mongodb.database\", mongo_db) \\\n",
    "    .option(\"spark.mongodb.collection\", mongo_collection) \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/checkpoint_pagos\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "PY_Asset_NRT",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
