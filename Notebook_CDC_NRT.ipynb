{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb525c62-ebef-43e4-be6a-c83288f3ce31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json, udf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, IntegerType, TimestampType\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "\n",
    "# 1. Cargar Archivo\n",
    "with open(\"config.json\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# 2. inicializaer Variables\n",
    "kafka_bootstrap_servers = config[\"kafkaCDC\"][\"bootstrap_servers\"]\n",
    "kafka_topic = config[\"kafkaCDC\"][\"topic\"]\n",
    "kafka_sasl_username = config[\"kafkaCDC\"][\"username\"]\n",
    "kafka_sasl_password = config[\"kafkaCDC\"][\"password\"]\n",
    "\n",
    "mongo_uri = config[\"mongodb\"][\"uri\"]\n",
    "mongo_db = config[\"mongodb\"][\"database\"]\n",
    "mongo_collection = config[\"mongodb\"][\"collection\"]\n",
    "\n",
    "# 3. Crear sesión de Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaToMongoBatchJob\") \\\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.0,\" \n",
    "            \"org.mongodb.spark:mongo-spark-connector_2.12:10.1.1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 4. Definir esquema del mensaje Kafka\n",
    "after_schema = StructType([\n",
    "    StructField(\"Id\", IntegerType()),\n",
    "    StructField(\"ClienteId\", StringType()),\n",
    "    StructField(\"Monto\", StringType()),\n",
    "    StructField(\"MetodoPago\", StringType()),\n",
    "    StructField(\"FechaPago\", StringType()),  # Llega como string\n",
    "    StructField(\"Estado\", StringType())\n",
    "])\n",
    "\n",
    "main_schema = StructType([\n",
    "    StructField(\"before\", StringType()),  # o StructType([]) si lo necesitas\n",
    "    StructField(\"after\", after_schema),\n",
    "    StructField(\"source\", StringType()),  # O StructType([]) si lo necesitas\n",
    "    StructField(\"op\", StringType()),\n",
    "    StructField(\"ts_ms\", LongType()),\n",
    "    StructField(\"transaction\", StringType())\n",
    "])\n",
    "\n",
    "# Función para convertir ticks de SQL Server a timestamp\n",
    "def ticks_to_timestamp(ticks_str):\n",
    "    try:\n",
    "        ticks = int(ticks_str)\n",
    "        epoch_start = datetime.datetime(1, 1, 1)\n",
    "        delta = datetime.timedelta(microseconds=ticks / 10)\n",
    "        return epoch_start + delta\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Registrar UDF\n",
    "ticks_to_timestamp_udf = udf(ticks_to_timestamp, TimestampType())\n",
    "\n",
    "# 5. Leer mensajes desde Kafka (modo streaming)\n",
    "kafka_options = {\n",
    "    \"kafka.bootstrap.servers\": kafka_bootstrap_servers,\n",
    "    \"subscribe\": kafka_topic,\n",
    "    \"kafka.security.protocol\": \"SASL_SSL\",\n",
    "    \"kafka.sasl.mechanism\": \"PLAIN\",\n",
    "    \"kafka.sasl.jaas.config\": f'org.apache.kafka.common.security.plain.PlainLoginModule required username=\"{kafka_sasl_username}\" password=\"{kafka_sasl_password}\";',\n",
    "    \"startingOffsets\": \"latest\"\n",
    "}\n",
    "df_kafka_streaming = spark.readStream.format(\"kafka\").options(**kafka_options).load()\n",
    "\n",
    "df_json = df_kafka_streaming.selectExpr(\"CAST(value AS STRING) as json_value\")\n",
    "# 6. Convertir 'value' a string y parsear JSON con el esquema\n",
    "df_parsed = df_json.select(from_json(col(\"json_value\"), main_schema).alias(\"data\")).select(\"data.after.*\")\n",
    "\n",
    "# 7. Convertir FechaPago\n",
    "df_final = df_parsed.withColumn(\"FechaPago\", ticks_to_timestamp_udf(col(\"FechaPago\")))\n",
    "\n",
    "# 8. Renombrar la columna 'Id' a '_id'\n",
    "df_mongo = df_final.withColumnRenamed(\"Id\", \"_id\")\n",
    "df_mongo.display()\n",
    "\n",
    "# 9. Escribir en MongoDB Atlas\n",
    "df_mongo.writeStream   \\\n",
    "    .format(\"mongodb\") \\\n",
    "    .option(\"spark.mongodb.connection.uri\", mongo_uri) \\\n",
    "    .option(\"spark.mongodb.database\", mongo_db) \\\n",
    "    .option(\"spark.mongodb.collection\", mongo_collection) \\\n",
    "    .option(\"checkpointLocation\", \"dbfs:/mnt/checkpoints/pagoscdc\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()\n",
    "\n",
    "#.option(\"checkpointLocation\", \"/tmp/checkpoint_pagos\") \\"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Notebook_CDC_NRT",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
